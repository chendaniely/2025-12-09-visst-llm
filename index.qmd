---
title: "A Practical Introduction to Large Language Models"
subtitle: "Vancouver Independent School for Science and Technology"
author: "Daniel Chen, MPH, PhD"

format:
  revealjs:
    footer: "[Daniel Chen](https://daniel.rbind.io/). @chendaniely. Repo/Slides: [https://github.com/chendaniely/2025-12-09-visst-llm](https://github.com/chendaniely/2025-12-09-visst-llm)"
    slide-number: c/t
    show-slide-number: all
    hash-type: number
    smaller: true

revealjs-plugins:
  - editable
filters:
  - editable

brand: _brand.yml

execute:
  echo: true
---

# Hello

## Daniel Chen

What I do

::::: columns
::: {.column width="33%"}
![](img/me.jpg){style="border-radius: 50%;"}
:::

::: {.column width="66%"}
- Lecturer, UBC Statistics, Master's of Data Science
- Developer Advocate, Posit, PBC
    - Talks, Workshops, Documentation
    - Shiny for Python
    - Get things working for the MDS team
:::
:::::

## Daniel Chen

What I've done

::::: columns
::: {.column width="33%"}
![](img/me.jpg){style="border-radius: 50%;"}
:::

::: {.column width="66%"}
- Macaulay Honors College CUNY Hunter College
    - Psychology + Neuroscience: Learning and memory
- Columbia University Mailman School of Public Health
    - Epidemiology: Spread of ideas in social networks
- Virginia Tech
    - Data science education in biomedical sciences

- The Carpentries, 2014
:::
:::::

## Daniel Chen {footer=false}

What I enjoy

::::: columns
::: {.column width="50%"}
![Daniel Chen](img/dan-run.jpg){style="float: right; height: 200px;"}
![Daniel Chen](img/dan-dive.jpg){style="float: right; height: 300px;"}
:::

::: {.column width="50%"}
![](img/dan-tour.jpeg){style="float: left; height: 525px;"}
:::
::::::

# A Practical Introduction to LLMs

## Poll: How skeptical are you about LLMs?

## You?

- Python (or some other programming language)
    - variables, functions, lists, loops, conditionals
    - objects, classes, methods, and attributes

- Familiar with Large Language Models (LLMs)
  - ChatGPT
  - Claude?
  - Any others?

## Maybe you?

- Application programming interfaces (APIs)
- HTTP requests
- GitHub account
- Python virtual environments

## Today

- Know how LLM interactions work
- Set up a simple chat with an LLM using Python
- Give you a framework to build on

## Packages

::: {.columns}
::: {.column}
Python Chatlas

![](img/logo-chatlas.png){width="300"}

<https://posit-dev.github.io/chatlas/>

:::
::: {.column}
R Ellmer

![](img/logo-ellmer.png){width="300"}

<https://ellmer.tidyverse.org/>

:::
:::

# Let's chat

## Your first chat: Setup

```{python}
import os

import chatlas as clt
from dotenv import load_dotenv

load_dotenv()
```

## Your first chat: Provider

```{python}
chat = clt.ChatAnthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
```

## Your first chat: Chat

```{python}
chat.chat("what is the capital of the moon?")
```

## Your first chat: Turn
```{python}
chat.chat("are you sure?")
```

# Your turn!

## Your turn: Python environment

{{< countdown "5:00" >}}

```bash
python -m venv .venv
source .venv/bin/activate
pip install chatlas python-dotenv openai anthropic
```

## Your turn: API key

{{< countdown "2:00" >}}

- You should have an API key
- Do not share this, especially online

## Your turn: Chat!

{{< countdown minutes=5 >}}

Create 2 files

1. `.env`: stores the python environment variables

```
ANTHROPIC_API_KEY="your_api_key_here"
OPENAI_API_KEY="your_api_key_here"
```

2. `chat.py`: python script to run the chat

```{python}
#| output: false
#| results: 'hide'
#| warning: false
#| error: false
#| messages: 'hide'
{{< include code/01-first.py >}}
```

# Modifying behaviors

## The system prompt

- Instructions that you give the model to change its behavior.
- It's really one of the only ways you can change how a model will respond.
- If you see a result you want to tweak or specify more, modify the system prompt!
- System prompts will grow as you use your model
- Prompts may change as you use different models (or update models)

## System prompt example

```{python}
{{< include code/02-prompt.py >}}
```

## Prompts are usually long

- Usually you save your prompt into a separate file (usually in markdown format)
- LLMs actually like it when you use a bit of formatting
- You can even use a bit of `{{templating}}` to customize prompts

## Example: File w/ templates

::: {.columns}
::: {.column}
Prompt file:

```markdown
{{< include code/prompt.md >}}
```
:::
::: {.column}
Code:

```{python}
#| output: false
{{< include code/03-prompt-read.py >}}
```
:::
:::

```{python}
#| echo: false
chat.get_last_turn().completion.content[0].text
```

## Your turn!

{{< countdown minutes=5 >}}

::: {.columns}
::: {.column}
Prompt file:

```markdown
{{< include code/prompt.md >}}
```
:::
::: {.column}
Code:

```python
{{< include code/03-prompt-read.py >}}
```
:::
:::

# Anatomy of a conversation

## Example Conversation

::: {style="text-align: right;"}
"What's the capital of the moon?"
:::

. . .

`"There isn't one."`

. . .

::: {style="text-align: right;"}
"Are you sure?"
:::

. . .

`"Yes, I am sure."`

## Example Request

```{.bash code-line-numbers="|5|6-9|7|8"}
curl https://api.openai.com/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -d '{
    "model": "gpt-4.1",
    "messages": [
        {"role": "system", "content": "You are a terse assistant."},
        {"role": "user", "content": "What is the capital of the moon?"}
    ]
}'
```
- Model: model used
- System prompt: behind-the-scenes instructions and information for the model
- User prompt: a question or statement for the model to respond to

## Example Response

Abridged response:

```{.json code-line-numbers="|3-6|7|12"}
{
  "choices": [{
    "message": {
      "role": "assistant",
      "content": "The moon does not have a capital. It is not inhabited or governed.",
    },
    "finish_reason": "stop"
  }],
  "usage": {
    "prompt_tokens": 9,
    "completion_tokens": 12,
    "total_tokens": 21,
    "completion_tokens_details": {
      "reasoning_tokens": 0
    }
  }
}
```

- Assistant: Response from model
- Why did the model stop responding
- Tokens: "words" used in the input and output

## Example Followup Request

```{.bash code-line-numbers="|9|10"}
curl https://api.openai.com/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -d '{
    "model": "gpt-4.1",
    "messages": [
      {"role": "system", "content": "You are a terse assistant."},
      {"role": "user", "content": "What is the capital of the moon?"},
      {"role": "assistant", "content": "The moon does not have a capital. It is not inhabited or governed."},
      {"role": "user", "content": "Are you sure?"}
    ]
}'
```

- The entire history is re-passed into the request

## Example Followup Response

Abridged Response:

```{.json code-line-numbers="|3-6|10-12"}
{
  "choices": [{
    "message": {
      "role": "assistant",
      "content": "Yes, I am sure. The moon has no capital or formal governance."
    },
    "finish_reason": "stop"
  }],
  "usage": {
    "prompt_tokens": 52,
    "completion_tokens": 15,
    "total_tokens": 67,
    "completion_tokens_details": {
      "reasoning_tokens": 0
    }
  }
}
```

. . .

Previous usage:

```{.json code-line-numbers="2-4"}
  "usage": {
    "prompt_tokens": 9,
    "completion_tokens": 12,
    "total_tokens": 21,
```

## Tokens

- Fundamental units of information for LLMs
- Words, parts of words, or individual characters

- Important for:
  - Model input/output limits
  - API pricing is usually by token
    - <https://gptforwork.com/tools/openai-chatgpt-api-pricing-calculator>

Try it yourself:

- <https://tiktokenizer.vercel.app/>
- <https://platform.openai.com/tokenizer>

## Token example

Common words represented with a single number:

:::{.incremental}
- What is the capital of the moon?
- 4827, 382, 290, 9029, 328, 290, 28479, 30
- 8 tokens total (including punctuation)
:::

. . .

Other words may require multiple numbers

:::{.incremental}
- counterrevolutionary
- counter, re, volution, ary
- 32128, 264, 9477, 815
- 4 tokens total
- 2-3 Tokens ﷺ (Arabic)
:::

## Token pricing (Anthropic)

<https://www.anthropic.com/pricing> -> API tab

::: {.columns}
::: {.column width="60%"}
![](/img/anthropic-pricing.png)
:::
::: {.column width="40%"}
Claude Sonnet 4

- Input: $3 / million tokens
- Output: $15 / million tokens
- Context window: 200k
:::
:::

## Context window

- Determines how much input can be incorporated into each output
- How much of the current history the agent has in the response

For Claude Sonnet:

- 200k token context window
- 150,000 words / 300 - 600 pages / 1.5 - 2 novels
- "Gödel, Escher, Bach" ~ 67,755 words

## Context window - chat history

200k tokens *seems* like a lot of context...

. . .

... but the entire chat is passed along each chat iteration

```json
{"role": "system", "content": "You are a terse assistant."},
{"role": "user", "content": "What is the capital of the moon?"},
{"role": "assistant", "content": "The moon does not have a capital. It is not inhabited or governed."},
{"role": "user", "content": "Are you sure?"},
{"role": "assistant", "content": "Yes, I am sure. The moon has no capital or formal governance."}
```

# Free?

## Github Models

- Runs models on Microsoft servers
- Has access to OpenAI models (and many others)
- Free tier available on a rate limit

- You will need to create a GitHub Personal Access Token (PAT).
    - It does not need any context (e.g., repo, workflow, etc).

## Example: Github model

Save it into an environment variable, `GITHUB_TOKEN`

<https://github.com/marketplace?type=models>

```{python}
{{< include code/04a-github.py >}}
```

## Github default model

```{python}
chat.get_turns()
```

## Ollama

- Completely offline
- Only real option for guaranteed privacy
- Models aren't as good as frontier models
- Some models take up a lot of space

```bash
ollama run qwen3:0.6b
```

## Example: Ollama

```{python}
{{< include code/04b-ollama.py >}}
```

# Tools and functions

## What are tools?

- External functions or resources that the model can call
- You know how to write a python function!

## Register tools (Agents)

```python
{{< include code/05-tool.py >}}
```

## Register tools (Agents)

```{python}
#| echo: false
{{< include code/05-tool.py >}}
```

# Summary

## Review some vocabulary

- `system prompt`: instructions to the model to modify its behavior
- `user prompt`: a question or statement for the model to respond to
- `assistant`: the model's response
- `tokens`: fundamental units of information for LLMs
- `context window`: how much input can be incorporated into each output
- `tool`: external function or resource that the model can call

## More vocabulary

- `agent`: an LLM that can use tools to accomplish tasks
  - Like a chatbot that can also use a calculator, search the web, or access your files
- `mcp`: Model Context Protocol: standarized way for you to talk to other services (usually tools)
  - It's like an standarized API for LLMs to talk to tools created by different people
- `RAG`: Retrieval-Augmented Generation: technique where the model retrieves relevant information from a database or document collection to enhance its responses
  - Think of doing natural language search on your own documents and then using results into your context.

# Next time?

## Thanks! {footer=false}

<https://github.com/chendaniely/2025-12-09-visst-llm>
